---
title: "HW 6"
author: "Shisham Adhikari"
date: "3/15/2020"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

###9.1.1
Given : X have the exponential distribution with parameter $\beta$. We wish to test the hypotheses $H_0 :β ≥ 1$ versus $H_1 : β < 1$. The test procedure $\delta$ rejects $H_0$ if $X ≥ 1$. Thus, the power function of the test is given by $\pi(\beta|\delta) = P_\beta(X ≥ 1) = \int_1^\infty \beta e^{-x\beta}dx = e^{-\beta}$. The size of the test is given by $\underset{\beta ≥ 1}{sup} \ \pi(\beta|\delta)$ = $\underset{\beta ≥ 1}{sup} \ e^{-\beta} = e^{-1}$, because $e^{-\beta}$ is a decreasing function of $\beta$. 

###9.1.2
Given that $X_1, \cdots, X_n$ form a random sample from the uniform distribution on the interval $[0, \theta]$, and that the hypotheses $H_0: θ≥2, H_1: θ<2$ are to be tested. Also, $Y_n = max [X_1, \cdots, X_n]$, and we are considering a test procedure such that the critical region contains all the outcomes for which $Y_n ≤ 1.5$. Thus, the power function of the test is given by $\pi(\theta|\delta) = P_\theta(Y_n ≤ 1.5) =  P_\theta(X_{max} ≤ 1.5)^n = \left(\frac{1.5}{\theta}\right)^n$ for $\theta>1.5$ and $\pi(\theta|\delta) = P_\theta(Y_n ≤ 1.5)=1$  for $\theta≤1.5$. \
The size of the test is $\underset{θ≥2}{sup} \left(\frac{1.5}{\theta}\right)^n = \left(\frac{1.5}{2}\right)^n =  \left(\frac{3}{4}\right)^n$, because $\pi(\theta|\delta)$ is a decreasing function of $\theta$. 

###9.1.14
Given $X_1, \cdots, X_n$ are i.i.d. with the exponential distribution with parameter $θ$. We wish to test the hypotheses $H_0: θ ≥θ_0, H_1: θ < θ_0$. $X= \sum_{i=1}^n X$, this means X $\sim \Gamma(n,\theta)$ and $\theta X \sim \Gamma(n,1)$. Also $δ_c$ be the test that rejects $H_0$ if $X≥c$. 

a. The power function $π(θ|δ_c) = P_\theta(X ≥ c) = P_\theta(\theta X ≥ \theta c)$ = 1-G(\theta c), where G is the CDF of $\Gamma(n,1)$ distribution and hence is a non-decreasing function of $\theta c$. So when $\theta$ goes up, G(\theta c) goes up and hence the power function, $π(θ|δ_c) = 1-G(\theta c)$ goes down. Thus, $π(θ|δ_c)$ is a decreasing function of $θ$. 

b. Since, the power function $π(θ|δ_c)$ is a decreasing function of $θ$, the maximum value of the power function in the space of the space of the null hypothesis $H_0: θ ≥θ_0$ occurs at $θ = θ_0$.So, the size of the test is $1-G(\theta_0 c)$. In order to make $δ_c$ have size $α_0$, we solve for  $1-G(\theta_0 c) = α_0$. Solving this, we get, $c = \frac{G^{-1}(1-\alpha_0)}{\theta_0}$, which is the required value. 

c. Given $θ_0 =2$,n=1,and $α_0 = 0.1$. We know, the CDF of $\Gamma(n,1)$ is $G(x) = 1-e^{-x}$, so $G^{-1}(x) = -ln(1-x)$, If so, the precise form of the test $δ_c$ is given by $c = \frac{G^{-1}(1-0.1)}{2} = \frac{-ln(1-1+0.1)}{2} = \frac{-ln(0.1)}{2} = -1.151$, which will give the precise form of the test. Its power function is $π(θ|δ_c) = 1-G(\theta c) = 1-G(-1.151\theta) = 1-1+e^{-1.151\theta} = e^{-1.151\theta}$. Plotting this power function, we get, 

```{r}
curve(0.1^(x/2), from=1, to=10, xlab="theta", ylab="power function, π(θ|δc)")
```


###9.2.2
Two p.d.f.’s $f_0(x)$ and $f_1(x)$ are given.
  
a. For a test procedure for which the value of $α(δ) + 2β(δ)$ is a minimum, we use the theorem on 9.2.1 that says the hypothesis $H_0$ is not rejected if $af_0(x) > bf_1(x)$ = $f_0(x) > 2f_1(x)$ =$1 > 2.2x = \frac{1}{4}>x$ and the hypothesis $H_0$ is rejected if $af_0(x) < bf_1(x)$ = $f_0(x) < 2f_1(x)$ =$1 < 2.2x = \frac{1}{4}<x$. The null hypothesis $H_0$ can be either rejected or not if $af_0(x) = bf_1(x)$ i.e. x = $\frac{1}{4}$. 
Thus, the required test $\delta$ test rejects the null if $\frac{1}{4}>X$, accepts the null if $\frac{1}{4}<X$, and can either accept or reject the null if $\frac{1}{4}=X$.

b. To determine the minimum value of $α(δ) + 2β(δ)$ attained by that procedure, $α(δ) = Pr(Reject \ H_0|f_0) = \int_{\frac{1}{4}}^1 f_0(x)dx = \frac{3}{4}$ and $\beta(δ) = Pr(Accept \ H_0|f_1) = \int_0^{\frac{1}{4}} f_1(x)dx = \int_0^{\frac{1}{4}} 2xdx = \frac{1}{16}$. Thus, the required minimum value is $\frac{3}{4} + 2\frac{1}{16} = \frac{7}{8}$.

###9.2.3
Consider again the conditions of Exercise 2, but suppose now that it is desired to find a test procedure for which the value of $3α(δ) + β(δ)$ is a minimum.

a. For a test procedure for which the value of $3α(δ) + β(δ)$ is a minimum, we again use the theorem on 9.2.1 that says the hypothesis $H_0$ is not rejected if $af_0(x) > bf_1(x)$ = $3f_0(x) > f_1(x)$ =$3 > 2x = \frac{3}{2}>x$ and the hypothesis $H_0$ is rejected if $3f_0(x) < f_1(x)$  =$3 < 2x = \frac{3}{2}<x$. The null hypothesis $H_0$ can be either rejected or not if $3f_0(x) = f_1(x)$ i.e. x = $\frac{3}{2}$. However, note that the sample space of X is [0,1], so this test procedure never rejects the null hypothesis and the optimal procedure is to accept the null for every possible value.  

b. Since the procedure never rejects the null, α(δ) = 0, β(δ) = 1 and the minimum value of $3α(δ) + β(δ)$ = 3.0+1 = 1.

###9.2.10
Given that $X_1,\cdots, X_n$ form a random sample from the Poisson distribution with unknown mean $λ$. $λ_0$ and $λ_1$ are specified values such that $λ_1 > λ_0 > 0$, an it is desired to test the following simple hypotheses: $H_0: λ = λ_0$ and $H_1: λ = λ_1$.

a. Again using the theorem 9.2.1 with a=b=1, the optimal test procedure to reject the null $H_0$ is 
\begin{align*}
f_1(x) &> f_0(x) \\
\implies e^{-n\lambda_1}\frac{\lambda_1^{\sum x_i}}{\prod_{i=1}^n x_i!} &> e^{-n\lambda_0}\frac{\lambda_0^{\sum x_i}}{\prod_{i=1}^n x_i!}\\
\implies  e^{-\lambda_1}\lambda_1^{\overline{X_n}} &> e^{-\lambda_0}\lambda_0^{\overline{X_n}} \\
\implies -\lambda_1+\overline{X_n}log(\lambda_1) &> -\lambda_0+\overline{X_n}log(\lambda_0) \\
\implies  \lambda_0 - \lambda_1 &> \overline{X_n} (log\lambda_0-log\lambda_1) \\
\implies  \lambda_1 - \lambda_0 &< \overline{X_n} (log\lambda_1-log\lambda_0)\\
\implies \overline{X_n} &> \frac{\lambda_1 - \lambda_0}{log\lambda_1-log\lambda_0}
\end{align*}
Thus the value of $α(δ)+β(δ)$ is minimized by a test procedure which rejects $H_0$ when $X_n > c$.

b. We see from a, the value of c is $\frac{\lambda_1 - \lambda_0}{log\lambda_1-log\lambda_0}$.

c. For $\sum X_i = n\overline{X_n}$, nc = $\frac{n(\lambda_1 - \lambda_0)}{log\lambda_1-log\lambda_0}$. For $λ_0 = \frac{1}{4}$, $λ_1 = \frac{1}{2}$, and n = 20, nc = $\frac{20(\frac{1}{2}-\frac{1}{4})}{log(\frac{1}{2})-log(\frac{1}{4})} = 7.214$. Also, we know $\sum X_i \sim$ Poisson(20$\lambda$). If so, $\alpha(\delta)= P\left(\sum_{i=1}^{20} X_i > 7.214|λ_0 = \frac{1}{4}\right)$ = 0.1334 and $\beta(\delta)= P\left(\sum_{i=1}^{20} X_i < 7.214|λ_1 = \frac{1}{2}\right)$ = 0.2203. So, the minimum value of $α(δ) + β(δ)$ = 0.1334 + 0.2203 = 0.3537.

###9.3.1
Given that $X_1,\cdots, X_n$ form a random sample from the Poisson distribution with unknown mean $λ (λ > 0)$. We need to show that the joint p.f. of $X1,\cdots, Xn$ has a monotone likelihood ratio in the statistic $\sum_{i=1}^n X$. Let T=f(x) be a statistic. Then, 
For some $\lambda_0< \lambda_1$, $\frac{f(x|\lambda = \lambda_1)}{f(x|\lambda = \lambda_0)}$ = $\frac{e^{-n\lambda_1}\frac{\lambda_1^{\sum X_i}}{\prod_{i=1}^n x_i!}}{e^{-n\lambda_0}\frac{\lambda_0^{\sum X_i}}{\prod_{i=1}^n x_i!}}$ = $exp(-n(\lambda_1-\lambda_0))\left(\frac{\lambda_1}{\lambda_0}\right)^{\sum X_i}$, where T = ${\sum X_i}$ is sufficient.  $\frac{f(\underline{x}|\lambda_1)}{f(\underline{x}|\lambda_0)}$ depends on $\underline{x}$ only through T and is a nondecreasing function of T over the range of T. Thus, the joint pmf of $X_1,\cdots, X_n$ has a monotone likelihood ratio in the statistic $\sum_{i=1}^n X_i$. 

###9.3.2
Given that $X_1,\cdots, X_n$ form a random sample from the normal distribution with known mean $μ$ and unknown variance $σ^2 (σ^2 > 0)$. Let the joint p.d.f. of the observations $X = (x_1,\cdots,x_n)$ is $f_n(x|\theta)$. Also let $T(x) = \sum_{i=1}^n (X_i - \mu)^2$ be the given statistic so that for some $\sigma_0< \sigma_1$, $\frac{f(x|\sigma = \sigma_1)}{f(x|\sigma = \sigma_0)}$ = $\frac{\frac{1}{\sigma_1^n}exp\left(-\frac{1}{2\sigma_1^2}T\right)}{\frac{1}{\sigma_0^n}exp\left(-\frac{1}{2\sigma_0^2}T\right)}$ = $\left(\frac{\sigma_0}{\sigma_1}\right)^n exp \left(\frac{T}{2}\left[\frac{1}{\sigma_0^2}-\frac{1}{\sigma_1^2}\right]\right)$, where T is sufficient and also is a decreasing function of T over the range of T. Thus, the joint pdf of $X_1,\cdots,X_n$ has a monotone likelihood ratio in the statistic T.  

###9.3.13
Given that four observations are taken at random from the normal distribution with unknown mean $μ$ and known variance 1, this has an MLR in $\overline{X_n}$.  Also the hypotheses $H_0: μ ≥ 10$ and $H_1: μ < 10$ are to be tested.

  a. To find a UMP test at the level of significance $α_0=0.1$, it rejects the null hypothesis when $\overline{X_n}<c$, where c is given by the level condition at $\mu=10$ i.e.$P_{\mu=10}(\overline{X_n} \leq c)= \alpha_0=0.1$. Here n=4 when $\mu=10$ and the r.v. $Z=2(\overline{X_n}-10) \sim N(0,1)$ with 
$P_{\mu=10}(\overline{X_n} \leq c) = Pr(Z \leq 2(c-10))$ which from the table is equal to $Pr(Z \leq -1.282)$ = 0.1. This means, 2(c-10) = 0.1 $\implies$ c = 9.359. Thus, the required UMP test at the level of significance $α_0=0.1$ is the one that rejects the null hypothesis if $\overline{X_n}<9.359$.
b. When $\mu=9$, the power function is given by  $\pi(\mu|\delta) = P_\mu(\overline{X_n} \leq c=9.359) = Pr(Z \leq 0.718) = \phi(0.718)=0.7636$.
c. The probability of not rejecting $H_0 \ if \ μ =11$ is $\pi(11|\delta) = P_11(\overline{X_n} \geq c=9.359) = Pr(Z \geq -3.282) = Pr(Z \leq 3.282) = \phi(3.282)=0.9995$